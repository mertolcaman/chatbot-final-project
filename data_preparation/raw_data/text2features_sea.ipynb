{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4951f04e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "import json\n",
    "import re\n",
    "from collections import defaultdict, OrderedDict\n",
    "\n",
    "# Load your API key from .env\n",
    "load_dotenv()\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "51b19d4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sea/dikili_bays.txt', 'sea/izmir_beaches.txt', 'sea/izmir_beaches_summary.txt']\n"
     ]
    }
   ],
   "source": [
    "folder_path = \"sea\"\n",
    "file_names = os.listdir(folder_path)\n",
    "\n",
    "file_names = [\n",
    "    folder_path + \"/\" + f\n",
    "    for f in file_names\n",
    "    if os.path.isfile(folder_path + \"/\" + f) and f.endswith(\".txt\")\n",
    "]\n",
    "\n",
    "print(file_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "51ab62bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_file_for_features(filepath: str):\n",
    "    #get the file content\n",
    "    with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "        raw_text = f.read()\n",
    "    \n",
    "    #model request\n",
    "    def modelRequest(prompt,temperature=0.5):\n",
    "\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=temperature\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "\n",
    "    \n",
    "    #String Response to JSON file conversion\n",
    "    def stringjson2json(output_text: str):\n",
    "        \"\"\"\n",
    "        Extracts and parses a JSON array from an OpenAI LLM string output,\n",
    "        which may contain markdown formatting, extra text, or surrounding commentary.\n",
    "\n",
    "        Returns:\n",
    "            - A parsed Python list/dictionary if successful\n",
    "            - None if parsing fails\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Try to find the first code block starting with ```json\n",
    "            if \"```json\" in output_text:\n",
    "                start = output_text.find(\"```json\") + len(\"```json\")\n",
    "                end = output_text.find(\"```\", start)\n",
    "                json_str = output_text[start:end].strip()\n",
    "            elif \"```\" in output_text:  # Fallback to any code block\n",
    "                start = output_text.find(\"```\") + len(\"```\")\n",
    "                end = output_text.find(\"```\", start)\n",
    "                json_str = output_text[start:end].strip()\n",
    "            else:\n",
    "                json_str = output_text.strip()\n",
    "\n",
    "            # Parse the JSON string\n",
    "            parsed = json.loads(json_str)\n",
    "\n",
    "            return parsed\n",
    "\n",
    "        except Exception as e:\n",
    "            print(\"JSON Parse Error:\", e)\n",
    "            return None\n",
    "\n",
    "    \n",
    "    # Step 1: Coreference resolution\n",
    "    def build_coref_prompt(text: str) -> str:\n",
    "        return f\"\"\"\n",
    "    Resolve all coreferences in the text below. Replace pronouns (he, she, it, there, this, etc.) \n",
    "    and indirect references with the appropriate named entities to make the text fully self-contained.\n",
    "\n",
    "    example-1:\n",
    "    Original: \"Elon Musk was born in South Africa. There, he briefly attended classes at the University of Pretoria.\"\n",
    "    After Coreference Resolution: \"Elon Musk was born in South Africa. In South Africa, Elon Musk briefly attended classes at the University of Pretoria.\"\n",
    "\n",
    "    example-2:\n",
    "    Original: \"İzmir is a bustling city on the Aegean coast. It is known for its vibrant culture and seaside promenades.\"\n",
    "    After Coreference Resolution: \"İzmir is a bustling city on the Aegean coast. İzmir is known for İzmir's vibrant culture and seaside promenades.\"\n",
    "\n",
    "    example-3:\n",
    "    Original: \"Cappadocia is famous for its fairy chimneys. They were formed over thousands of years by volcanic activity and erosion.\"\n",
    "    After Coreference Resolution: \"Cappadocia is famous for Cappadocia's fairy chimneys. Cappadocia's fairy chimneys were formed over thousands of years by volcanic activity and erosion.\"\n",
    "\n",
    "    example-4:\n",
    "    Original: \"Pamukkale is known for its white travertine terraces. They are formed by mineral-rich hot springs.\"\n",
    "    After Coreference Resolution: \"Pamukkale is known for Pamukkale's white travertine terraces. Pamukkale's white travertine terraces are formed by mineral-rich hot springs.\"\n",
    "\n",
    "    Text:\n",
    "    {text}\n",
    "\n",
    "    Rewritten Text:\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    resolved_text = modelRequest(build_coref_prompt(raw_text), temperature=0.5)\n",
    "    \n",
    "    print(\"Step-1: Coreference resolution has been done.\")\n",
    "    # Step 2: Chunking\n",
    "    def chunk_text_prompt(text: str) -> str:\n",
    "        return f\"\"\"\n",
    "    You are an expert knowledge extraction assistant specializing in travel content and knowledge graph construction.\n",
    "\n",
    "    Your task is to help organize a long travel-related text by **chunking it into meaningful sections** based on shifts in topic or place. These shifts are usually marked by **headers/titles**, such as the name of a city, region, or town.\n",
    "\n",
    "    Instructions:\n",
    "    - Read the input text.\n",
    "    - When you detect a shift in the topic (typically marked by a new place or region title), split the text.\n",
    "    - For the first section before the next topic begins:\n",
    "        - Assign the nearest title (place name) as the \"Place\".\n",
    "        - Extract all relevant text until the next shift.\n",
    "        - Return this section as the first chunk.\n",
    "    - Leave the remaining text under \"Other part\" so we can continue chunking recursively.\n",
    "\n",
    "    Do not return multiple chunks at once.\n",
    "    Always return only the first chunk and the remaining part.\n",
    "\n",
    "    Return your output in the following structured format:\n",
    "\n",
    "    Place: <Place Name>\n",
    "    Chunk: <Chunked Text>\n",
    "    Other part: <Remaining Text>\n",
    "    ---\n",
    "    Here is the text:\n",
    "    \\\"\\\"\\\"{text}\\\"\\\"\\\"\n",
    "    \"\"\"\n",
    "    def get_place_and_chunk(chunked_response):\n",
    "        place_match = re.search(r\"Place:\\s*(.+?)\\s*Chunk:\", chunked_response, re.DOTALL)\n",
    "        chunk_match = re.search(r\"Chunk:\\s*(.+?)\\s*Other part:\", chunked_response, re.DOTALL)\n",
    "\n",
    "        return {\n",
    "            \"place\": place_match.group(1).strip(),\n",
    "            \"chunk\": chunk_match.group(1).strip()\n",
    "        }\n",
    "\n",
    "\n",
    "\n",
    "    chunks=[]\n",
    "    remaining_text=resolved_text\n",
    "\n",
    "    while True:\n",
    "        prompt=chunk_text_prompt(remaining_text)\n",
    "        chunked_response=modelRequest(prompt,temperature=0.2)\n",
    "\n",
    "        chunked_part=get_place_and_chunk(chunked_response)\n",
    "        chunks.append(chunked_part)\n",
    "\n",
    "        last_chunked_text=chunked_part[\"chunk\"][-50:]\n",
    "#         print(\"last chunked text: \",last_chunked_text)\n",
    "        find_end_index=re.search(last_chunked_text, resolved_text).span()[1]\n",
    "#         print(\"find end index: \",find_end_index)\n",
    "        remaining_text=resolved_text[find_end_index+1:]\n",
    "#         print(\"remaining text: \", remaining_text)\n",
    "        if len(remaining_text)<5:\n",
    "            break   \n",
    "\n",
    "    print(\"Step-2: Chunking has been done.\")        \n",
    "    # Step 3: Canonical NER\n",
    "    \n",
    "    def text2CanonicalNER(place: str, chunked_text: str) -> str:\n",
    "        return f\"\"\"\n",
    "    You are an expert assistant specializing in building travel knowledge graphs.\n",
    "\n",
    "    Your task is to extract **all named entities** from the following paragraph related to the place: **\"{place}\"**.\n",
    "\n",
    "    For each identified entity, return a JSON object with the following fields:\n",
    "    - **\"mention\"**: the exact text as it appears in the paragraph.\n",
    "    - **\"label\"**: the semantic type (choose from: LOCATION, LANDMARK, HISTORICAL_SITE, NATURAL_SITE, EVENT, CULTURAL_SITE).\n",
    "    - **\"canonical\"**: the normalized, standardized, or modern name (e.g., internationally recognized or geolocated version).\n",
    "    - **\"note\"** *(optional)*: include only if there's something notable about the entity such as:\n",
    "      - historical origin (e.g., Greek, Ottoman),\n",
    "      - potential ambiguity (e.g., multiple interpretations),\n",
    "      - linguistic note (e.g., old name or foreign origin).\n",
    "\n",
    "    Please ensure:\n",
    "    - Each entity is related to **travel**, **history**, or **geography**.\n",
    "    - Group output as a **JSON array** and return **only** the JSON, without extra explanation.\n",
    "\n",
    "    ---\n",
    "    Here is the paragraph related to \"{place}\":\n",
    "    \\\"\\\"\\\"{chunked_text}\\\"\\\"\\\"\n",
    "    \"\"\"\n",
    "    canonical_entities=[]\n",
    "    for chunk in chunks:\n",
    "        canonical_prompt=text2CanonicalNER(chunk[\"place\"], chunk[\"chunk\"])\n",
    "        canonical_response=modelRequest(canonical_prompt,temperature=0.2)\n",
    "    #     canonical_entities.extend(stringjson2json(canonical_response))\n",
    "        canonical_entities.append(stringjson2json(canonical_response))\n",
    "        print(f'{chunk[\"place\"]} has been done.')\n",
    "\n",
    "\n",
    "    print(\"Step-3: Canonical NER has been done.\")    \n",
    "    # Step 4: Feature Extraction\n",
    "    def text2feature(parsed_json: list, text: str) -> str:\n",
    "        return f\"\"\"\n",
    "You are a structured knowledge extraction assistant for a smart travel recommendation system.\n",
    "\n",
    "Below is a list of identified entities (e.g., towns, beaches, ruins) extracted from a travel paragraph, including their labels and notes:\n",
    "\n",
    "{json.dumps(parsed_json, indent=2)}\n",
    "\n",
    "Here is the paragraph they were extracted from:\n",
    "\n",
    "\\\"\\\"\\\"{text}\\\"\\\"\\\"\n",
    "\n",
    "Your task is to enrich each entity using only the information explicitly or implicitly present in the paragraph.\n",
    "\n",
    "For each entity, extract the following fields:\n",
    "- \"mention\": the exact name from the list above\n",
    "- \"node_type\": choose the most appropriate type from this controlled list:\n",
    "\n",
    "  [City, Region, Town, Village, Beach, Bay, Island, Museum, HistoricalSite, ReligiousPlace, Market, NaturalPark, AncientCity, Monument, Castle, Temple, Tower, Bridge]\n",
    "\n",
    "  Refer to the guide below to choose correctly:\n",
    "  - City: major urban center with administrative or economic importance\n",
    "  - Region: geographic or administrative area (e.g., peninsula, province)\n",
    "  - Town: medium-sized settlement\n",
    "  - Village: small, often rural settlement\n",
    "  - Beach: coastal area suitable for swimming or recreation\n",
    "  - Bay: curved coastal inlet or small cove (includes places ending with “-bükü”)\n",
    "  - Island: landmass surrounded by water\n",
    "  - Museum: institution showcasing cultural or historical artifacts\n",
    "  - HistoricalSite: ruins, ancient structures, castles, or aqueducts\n",
    "  - ReligiousPlace: places of worship such as churches, mosques, or temples\n",
    "  - Market: traditional, open-air, or specialty shopping area\n",
    "  - NaturalPark: protected or scenic natural area\n",
    "  - AncientCity: historic archaeological city or urban ruin\n",
    "  - Monument: statue, memorial, or commemorative landmark\n",
    "  - Castle: large fortified structure, often historical\n",
    "  - Temple: structure for religious or spiritual rituals\n",
    "  - Tower: tall structure such as a lighthouse or watchtower\n",
    "  - Bridge: architectural structure spanning a natural feature\n",
    "\n",
    "Do not invent new types. Be precise and pick the closest match based on the paragraph context.\n",
    "\n",
    "Also extract:\n",
    "- \"what_to_do\": list of activities possible at or near this location (e.g., swim, photograph, explore, relax)\n",
    "- \"best_for\": ideal traveler types (e.g., couples, families, nature lovers, cultural tourists)\n",
    "- \"special_features\": unique aspects or highlights (e.g., history, architecture, scenic views)\n",
    "- \"tags\": short, descriptive keywords (e.g., beach, ancient ruins, sunset view, bazaar)\n",
    "- \"description\": 1–2 sentence summary describing the place to a traveler\n",
    "- \"located_in\": the **smallest known region mentioned** in the paragraph (e.g., if a beach is located in a village called “Sığacık” which is part of Seferihisar, use `\"Sığacık\"`. If there is no clue, then add the relevant location about the place)\n",
    "Respond with a **valid JSON list**, with one object per entity in the same order.\n",
    "\n",
    "Example output format:\n",
    "\n",
    "[\n",
    "  {{\n",
    "    \"mention\": \"Blue Lagoon\",\n",
    "    \"node_type\": \"Beach\",\n",
    "    \"what_to_do\": [\"swimming\", \"snorkeling\", \"boat tours\"],\n",
    "    \"best_for\": [\"families\", \"nature lovers\", \"couples\"],\n",
    "    \"special_features\": [\"turquoise waters\", \"protected natural area\", \"mountain backdrop\"],\n",
    "    \"tags\": [\"beach\", \"lagoon\", \"scenic\", \"family-friendly\"],\n",
    "    \"description\": \"Blue Lagoon is a serene coastal destination with clear turquoise waters and a calm environment, ideal for swimming and relaxation.\"\n",
    "    \"located_in\": \"Ölüdeniz\" \n",
    " }}\n",
    "]\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "    last_features=[]\n",
    "    for entity,chunk in zip(canonical_entities, chunks):\n",
    "        feature_prompt=text2feature(entity, chunk[\"chunk\"])\n",
    "        feature_response=modelRequest(feature_prompt,temperature=0.2)\n",
    "    #     canonical_entities.extend(stringjson2json(canonical_response))\n",
    "        last_features.append(stringjson2json(feature_response))\n",
    "        \n",
    "    print(\"Step-4: Feature Extraction has been done.\")\n",
    "    # Step 5: Validation\n",
    "    def validate_extracted_features(entity_list: list, paragraph: str) -> str:\n",
    "        return f\"\"\"\n",
    "    You are an expert assistant reviewing enriched travel entities extracted from the paragraph below.\n",
    "\n",
    "    Check each entity for:\n",
    "    - Accuracy of the \"node_type\"\n",
    "    - Whether the fields (\"what_to_do\", \"best_for\", \"special_features\", \"tags\", \"description\") are supported by or inferred from the paragraph\n",
    "\n",
    "    If any entity contains unsupported, incorrect, or hallucinated values, **correct them**.\n",
    "\n",
    "    Return your answer as a **valid JSON array**, with the corrected entities. Do not include any explanation or review. Only output the corrected list.\n",
    "\n",
    "    Here is the paragraph:\n",
    "    \\\"\\\"\\\"{paragraph}\\\"\\\"\\\"\n",
    "\n",
    "    Here are the enriched entities to validate:\n",
    "    {json.dumps(entity_list, indent=2)}\n",
    "    \"\"\"\n",
    "    \n",
    "    validated_features=[]\n",
    "    for entity,chunk in zip(last_features, chunks):\n",
    "        validation_prompt=validate_extracted_features(entity, chunk[\"chunk\"])\n",
    "        feature_response=modelRequest(validation_prompt,temperature=0.2)\n",
    "        validated_features.extend(stringjson2json(feature_response))\n",
    "    print(\"Step-5: Validation of Extracted Features has been done.\")\n",
    "    \n",
    "    # Step 6: Merging the same Features\n",
    "\n",
    "    # Group and merge logic\n",
    "    def merge_dicts(existing, new):\n",
    "        for key, value in new.items():\n",
    "            if key in ['mention', 'node_type']:\n",
    "                continue\n",
    "            if isinstance(value, list):\n",
    "                existing[key].extend(value)\n",
    "            elif isinstance(value, str):\n",
    "                existing[key].append(value)\n",
    "        return existing\n",
    "    grouped = defaultdict(lambda: defaultdict(list))\n",
    "    mention_to_node_type = {}\n",
    "    # Use directly as it's already flat\n",
    "    for entry in validated_features:\n",
    "        mention = entry['mention']\n",
    "        mention_to_node_type[mention] = entry['node_type']\n",
    "        grouped[mention] = merge_dicts(grouped[mention], entry)\n",
    "    # Build the final merged list\n",
    "    merged_features = []\n",
    "    for mention, data in grouped.items():\n",
    "        ordered = OrderedDict()\n",
    "        ordered['mention'] = mention\n",
    "        ordered['node_type'] = mention_to_node_type[mention]\n",
    "        for key, value in data.items():\n",
    "            if key not in ['mention', 'node_type']:\n",
    "                if key == 'description':\n",
    "                    ordered[key] = \" \".join(set(value))  # Merge all descriptions\n",
    "                else:\n",
    "                    ordered[key] = list(set(value))  # Remove duplicates\n",
    "        merged_features.append(ordered)\n",
    "   \n",
    "\n",
    "    print(\"Step-6: Merging the same Features has been done.\")\n",
    "    return merged_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "86302e97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step-1: Coreference resolution has been done.\n",
      "Step-2: Chunking has been done.\n",
      "Dikili has been done.\n",
      "Step-3: Canonical NER has been done.\n",
      "Step-4: Feature Extraction has been done.\n",
      "Step-5: Validation of Extracted Features has been done.\n",
      "Step-6: Merging the same Features has been done.\n",
      "-------------------------------\n",
      "sea/dikili_bays.txt HAS BEEN PROCESSED!\n",
      "-------------------------------\n",
      "Step-1: Coreference resolution has been done.\n",
      "Step-2: Chunking has been done.\n",
      "Çeşme has been done.\n",
      "Seferihisar has been done.\n",
      "Foça has been done.\n",
      "Urla has been done.\n",
      "Alaçatı has been done.\n",
      "Dikili has been done.\n",
      "Karaburun has been done.\n",
      "Step-3: Canonical NER has been done.\n",
      "Step-4: Feature Extraction has been done.\n",
      "Step-5: Validation of Extracted Features has been done.\n",
      "Step-6: Merging the same Features has been done.\n",
      "-------------------------------\n",
      "sea/izmir_beaches.txt HAS BEEN PROCESSED!\n",
      "-------------------------------\n",
      "Step-1: Coreference resolution has been done.\n",
      "Step-2: Chunking has been done.\n",
      "Izmir has been done.\n",
      "Alaçatı has been done.\n",
      "Seferihisar has been done.\n",
      "Foça has been done.\n",
      "Urla has been done.\n",
      "Dikili has been done.\n",
      "Karaburun has been done.\n",
      "Izmir has been done.\n",
      "Step-3: Canonical NER has been done.\n",
      "Step-4: Feature Extraction has been done.\n",
      "Step-5: Validation of Extracted Features has been done.\n",
      "Step-6: Merging the same Features has been done.\n",
      "-------------------------------\n",
      "sea/izmir_beaches_summary.txt HAS BEEN PROCESSED!\n",
      "-------------------------------\n"
     ]
    }
   ],
   "source": [
    "for file_name in file_names:\n",
    "    validated_features=process_file_for_features(file_name)\n",
    "    with open(file_name[:-4]+\".json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(validated_features, f, indent=4, ensure_ascii=False)\n",
    "    print(\"-------------------------------\")\n",
    "    print(f\"{file_name} HAS BEEN PROCESSED!\")\n",
    "    print(\"-------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd2bd35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adeb5d25",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
