{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "123fbd03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "import json\n",
    "import re\n",
    "\n",
    "# Load your API key from .env\n",
    "load_dotenv()\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e8a5cbcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['food/izmir_foods.txt']\n"
     ]
    }
   ],
   "source": [
    "folder_path = \"food\"\n",
    "file_names = os.listdir(folder_path)\n",
    "\n",
    "file_names = [\n",
    "    folder_path + \"/\" + f\n",
    "    for f in file_names\n",
    "    if os.path.isfile(folder_path + \"/\" + f) and f.endswith(\".txt\")\n",
    "]\n",
    "\n",
    "print(file_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d680957f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "\n",
    "def build_coref_prompt(text: str) -> str:\n",
    "    return (\n",
    "        \"Resolve all coreferences in the text below. Replace pronouns (he, she, it, there, this, etc.) \"\n",
    "        \"and indirect references with the appropriate named entities to make the text fully self-contained.\\n\\n\"\n",
    "        \"example-1:\\n\"\n",
    "        \"Original: \\\"Elon Musk was born in South Africa. There, he briefly attended classes at the University of Pretoria.\\\"\\n\"\n",
    "        \"After Coreference Resolution: \\\"Elon Musk was born in South Africa. In South Africa, Elon Musk briefly attended classes at the University of Pretoria.\\\"\\n\\n\"\n",
    "        \"example-2:\\n\"\n",
    "        \"Original: \\\"İzmir is a bustling city on the Aegean coast. It is known for its vibrant culture and seaside promenades.\\\"\\n\"\n",
    "        \"After Coreference Resolution: \\\"İzmir is a bustling city on the Aegean coast. İzmir is known for İzmir's vibrant culture and seaside promenades.\\\"\\n\\n\"\n",
    "        f\"Text:\\n{text}\\n\\nRewritten Text:\"\n",
    "    )\n",
    "\n",
    "def chunk_text_prompt(text: str) -> str:\n",
    "    return f\"\"\"\n",
    "You are an assistant helping organize culinary information for a Smart Travel Planner in İzmir.\n",
    "\n",
    "Split the input text into chunks, each chunk describing **exactly one food or drink**.\n",
    "\n",
    "Return your output in this format:\n",
    "Food: <name of food>\n",
    "Chunk: <related description and features>\n",
    "Other part: <remaining text not yet chunked>\n",
    "\n",
    "Only return one food chunk at a time. If no more food remains, return:\n",
    "Food: None\n",
    "Chunk: None\n",
    "Other part: None\n",
    "\n",
    "Here is the food text:\n",
    "\\\"\\\"\\\"{text}\\\"\\\"\\\"\n",
    "\"\"\"\n",
    "\n",
    "def get_food_and_chunk(chunked_response):\n",
    "    food_match = re.search(r\"Food:\\s*(.+?)\\s*Chunk:\", chunked_response, re.DOTALL)\n",
    "    chunk_match = re.search(r\"Chunk:\\s*(.+?)\\s*Other part:\", chunked_response, re.DOTALL)\n",
    "    if not food_match or not chunk_match:\n",
    "        return None\n",
    "    return {\n",
    "        \"food\": food_match.group(1).strip(),\n",
    "        \"chunk\": chunk_match.group(1).strip()\n",
    "    }\n",
    "\n",
    "def stringjson2json(output_text: str):\n",
    "    try:\n",
    "        if \"```json\" in output_text:\n",
    "            start = output_text.find(\"```json\") + len(\"```json\")\n",
    "            end = output_text.find(\"```\", start)\n",
    "            json_str = output_text[start:end].strip()\n",
    "        elif \"```\" in output_text:\n",
    "            start = output_text.find(\"```\") + len(\"```\")\n",
    "            end = output_text.find(\"```\", start)\n",
    "            json_str = output_text[start:end].strip()\n",
    "        else:\n",
    "            json_str = output_text.strip()\n",
    "        return json.loads(json_str)\n",
    "    except Exception as e:\n",
    "        print(\"JSON Parse Error:\", e)\n",
    "        return None\n",
    "\n",
    "def text2feature(food_name: str, text: str) -> str:\n",
    "    return f\"\"\"\n",
    "You are a structured knowledge extraction assistant for a smart travel recommendation system.\n",
    "\n",
    "Below is a food-related mention extracted from a travel paragraph:\n",
    "\n",
    "\\\"\\\"\\\"{text}\\\"\\\"\\\"\n",
    "\n",
    "Your task is to enrich the food item using only the information explicitly or implicitly present in the paragraph.\n",
    "\n",
    "Extract the following fields:\n",
    "- \"name\": \\\"\\\"\\\"{food_name}\\\"\\\"\\\"\n",
    "- \"type\": one of [\"pastry\", \"sandwich\", \"dessert\", \"drink\", \"seafood\", \"offal dish\", \"street food\", \"snack\"]\n",
    "- \"ingredients\": list of key ingredients mentioned or clearly implied\n",
    "- \"description\": a concise, accurate summary (1–2 sentences)\n",
    "- \"where_to_eat\": list of named locations (neighborhoods, districts, streets) mentioned in the text where this food is commonly found\n",
    "\n",
    "Return a valid JSON list containing one object. Do not guess or invent locations.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def validate_food_features(entity_list: list, paragraph: str) -> str:\n",
    "    return f\"\"\"\n",
    "You are an expert assistant reviewing enriched food data extracted from a travel paragraph.\n",
    "\n",
    "Your task is to validate and correct each food entity based on the paragraph provided.\n",
    "\n",
    "For each entity, check:\n",
    "- Is the \"type\" appropriate based on the description?\n",
    "- Are the \"ingredients\" mentioned or strongly implied in the paragraph?\n",
    "- Is the \"description\" accurate, concise, and supported by the text?\n",
    "- Is the \"where_to_eat\" field consistent with place names mentioned in the paragraph? Remove hallucinated or vague locations.\n",
    "\n",
    "Fix any hallucinated or missing values.\n",
    "\n",
    "Return only a valid JSON list with the corrected fields.\n",
    "\n",
    "Here is the paragraph:\n",
    "\\\"\\\"\\\"{paragraph}\\\"\\\"\\\"\n",
    "\n",
    "Here is the extracted food entity:\n",
    "{json.dumps(entity_list, indent=2, ensure_ascii=False)}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def modelRequest(prompt, temperature=0.5):\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=temperature\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "def process_food_text(text: str):\n",
    "    # Step 1: Coreference resolution\n",
    "    coref_prompt = build_coref_prompt(text)\n",
    "    resolved_text = modelRequest(coref_prompt, temperature=0.5)\n",
    "    print(\"Step-1: Coreference resolution complete.\")\n",
    "\n",
    "    # Step 2: Chunking\n",
    "    chunks = []\n",
    "    remaining_text = resolved_text\n",
    "    while True:\n",
    "        prompt = chunk_text_prompt(remaining_text)\n",
    "        chunked_response = modelRequest(prompt, temperature=0.2)\n",
    "        chunked_part = get_food_and_chunk(chunked_response)\n",
    "\n",
    "        if not chunked_part or chunked_part[\"food\"].lower() == \"none\":\n",
    "            break\n",
    "        if chunks and chunks[-1][\"food\"].lower() == chunked_part[\"food\"].lower():\n",
    "            break\n",
    "\n",
    "        chunks.append(chunked_part)\n",
    "        print(f\"{chunked_part['food']} extracted.\")\n",
    "\n",
    "        last_chunked_text = chunked_part[\"chunk\"][-10:]\n",
    "        match = re.search(re.escape(last_chunked_text), remaining_text)\n",
    "        if match:\n",
    "            remaining_text = remaining_text[match.span()[1] + 1:]\n",
    "        else:\n",
    "            break\n",
    "    print(\"Step-2: Chunking complete.\")\n",
    "\n",
    "    # Step 3: Feature Extraction\n",
    "    last_features = []\n",
    "    for chunk in chunks:\n",
    "        food_name = chunk[\"food\"]\n",
    "        food_chunk = chunk[\"chunk\"]\n",
    "        feature_prompt = text2feature(food_name, food_chunk)\n",
    "        feature_response = modelRequest(feature_prompt, temperature=0.2)\n",
    "        parsed = stringjson2json(feature_response)\n",
    "        if parsed:\n",
    "            last_features.append(parsed)\n",
    "        else:\n",
    "            print(f\"Feature extraction failed for {food_name}\")\n",
    "    print(\"Step-3: Feature extraction complete.\")\n",
    "\n",
    "    # Step 4: Validation\n",
    "    validated_features = []\n",
    "    for to_validate, chunk in zip(last_features, chunks):\n",
    "        validation_prompt = validate_food_features(to_validate, chunk[\"chunk\"])\n",
    "        validation_response = modelRequest(validation_prompt, temperature=0.2)\n",
    "        corrected = stringjson2json(validation_response)\n",
    "        if corrected:\n",
    "            validated_features.extend(corrected)\n",
    "        else:\n",
    "            print(f\"Validation failed for {to_validate[0]['name']}\")\n",
    "    print(\"Step-4: Validation complete.\")\n",
    "    return validated_features\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a0f5a4be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step-1: Coreference resolution complete.\n",
      "Boyoz extracted.\n",
      "Gevrek extracted.\n",
      "Kumru extracted.\n",
      "Kokoreç extracted.\n",
      "Midye extracted.\n",
      "Söğüş extracted.\n",
      "Lokma extracted.\n",
      "Torpil extracted.\n",
      "Şambali extracted.\n",
      "Sübye Sherbet extracted.\n",
      "İzmir Bomba extracted.\n",
      "Step-2: Chunking complete.\n",
      "Step-3: Feature extraction complete.\n",
      "Step-4: Validation complete.\n",
      "-------------------------------\n",
      "food/izmir_foods.txt HAS BEEN PROCESSED!\n",
      "-------------------------------\n"
     ]
    }
   ],
   "source": [
    "for file_name in file_names:\n",
    "    with open(file_name, \"r\", encoding=\"utf-8\") as f:\n",
    "        input_text = f.read()\n",
    "    validated_features = process_food_text(input_text)\n",
    "    with open(file_name[:-4] + \".json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(validated_features, f, indent=4, ensure_ascii=False)\n",
    "    print(\"-------------------------------\")\n",
    "    print(f\"{file_name} HAS BEEN PROCESSED!\")\n",
    "    print(\"-------------------------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7524e20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0edb4f35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8450fda",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34836cb2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c1f4313",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
